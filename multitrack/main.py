"""
Point d'entr√©e principal du syst√®me de surveillance multi-cam√©ras.
Interface CLI pour l'analyse, la synchronisation et l'affichage des flux.
"""

import argparse
import glob
import os
import sys
import json
import time
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict

# Import des modules
from .video_metadata import extract_all_metadata, save_metadata_report, VideoMetadata
from .video_sync import VideoSynchronizer, SyncMethod, SynchronizedReader
from .stream_manager import StreamManager, CameraConfig
from .data_storage import DataStorage, VideoRecord
from .health_monitor import HealthMonitor, AlertLevel
from .sync_merger import SyncMerger, merge_videos_to_grid
from .time_sync_player import TimeSyncedPlayer, get_rotation_for_video
from .preprocessing import PreprocessingConfig, PreprocessingPipeline, OutputFormat
from .parallel_processor import ParallelProcessor, BatchProcessor
from .object_detector import ObjectDetector, create_surveillance_detector
from .object_tracker import ObjectTracker, create_surveillance_tracker
from .mcmot import MCMOTTracker, BEVVisualizer


def expand_video_patterns(patterns):
    """
    √âtend les patterns pour inclure les extensions majuscules/minuscules.
    Ex: Dataset/*.mp4 -> inclut aussi Dataset/*.MP4
    """
    video_paths = []
    for pattern in patterns:
        video_paths.extend(glob.glob(pattern))
        # Ajouter aussi la version majuscule/minuscule
        if pattern.endswith('.mp4'):
            video_paths.extend(glob.glob(pattern[:-4] + '.MP4'))
        elif pattern.endswith('.MP4'):
            video_paths.extend(glob.glob(pattern[:-4] + '.mp4'))
    # Supprimer les doublons
    return list(dict.fromkeys(video_paths))


def analyze_videos(args):
    """Analyse les m√©tadonn√©es des vid√©os existantes."""
    print("\nüìπ Analyse des vid√©os...\n")
    
    # Trouver les vid√©os (inclut .mp4 et .MP4)
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Vid√©os trouv√©es: {len(video_paths)}\n")
    
    # Extraire les m√©tadonn√©es
    metadata_list = extract_all_metadata(video_paths)
    
    # Afficher le r√©sum√©
    print(f"\n{'='*60}")
    print("üìä R√âSUM√â")
    print(f"{'='*60}")
    
    total_duration = sum(m.duration_seconds for m in metadata_list)
    total_frames = sum(m.frame_count for m in metadata_list)
    resolutions = set(f"{m.width}x{m.height}" for m in metadata_list)
    framerates = set(f"{m.fps:.2f}" for m in metadata_list)
    
    print(f"   Vid√©os analys√©es: {len(metadata_list)}")
    print(f"   Dur√©e totale: {total_duration/60:.1f} minutes")
    print(f"   Frames totales: {total_frames:,}")
    print(f"   R√©solutions: {', '.join(resolutions)}")
    print(f"   Framerates: {', '.join(framerates)} FPS")
    
    # Sauvegarder le rapport si demand√©
    if args.output:
        save_metadata_report(metadata_list, args.output)
        print(f"\n‚úì Rapport sauvegard√©: {args.output}")
    
    return metadata_list


def synchronize_videos(args):
    """Synchronise les vid√©os existantes."""
    print("\nüîÑ Synchronisation des vid√©os...\n")
    
    # Trouver les vid√©os (inclut .mp4 et .MP4)
    video_paths = expand_video_patterns(args.videos)
    
    if len(video_paths) < 2:
        print("‚ùå Au moins 2 vid√©os requises pour la synchronisation")
        return
    
    # Extraire les m√©tadonn√©es
    metadata_list = extract_all_metadata(video_paths)
    
    # Cr√©er le synchroniseur
    synchronizer = VideoSynchronizer(
        target_fps=args.target_fps,
        max_drift_ms=args.max_drift_ms
    )
    
    # Choisir la m√©thode
    method = SyncMethod(args.method)
    print(f"M√©thode: {method.value}\n")
    
    try:
        if method == SyncMethod.TIMESTAMP:
            results = synchronizer.synchronize_by_timestamp(metadata_list)
        elif method == SyncMethod.AUDIO:
            results = synchronizer.synchronize_by_audio(video_paths)
        elif method == SyncMethod.VISUAL:
            results = synchronizer.synchronize_by_visual_event(
                video_paths,
                event_type=args.event_type
            )
        else:
            print("‚ùå M√©thode non support√©e")
            return
        
        # Afficher les r√©sultats
        print(f"\n{'='*60}")
        print("üìä R√âSULTATS DE SYNCHRONISATION")
        print(f"{'='*60}")
        
        for r in results:
            print(f"\n   {r.camera_id}:")
            print(f"      Offset: {r.offset_seconds:.3f}s ({r.offset_frames} frames)")
            print(f"      Confiance: {r.confidence*100:.0f}%")
            print(f"      FPS: {r.original_fps:.2f} ‚Üí {r.target_fps:.2f}")
        
        # Sauvegarder les r√©sultats
        if args.output:
            sync_data = {
                "method": method.value,
                "target_fps": args.target_fps,
                "results": [
                    {
                        "camera_id": r.camera_id,
                        "video_path": r.video_path,
                        "offset_seconds": r.offset_seconds,
                        "offset_frames": r.offset_frames,
                        "confidence": r.confidence,
                    }
                    for r in results
                ]
            }
            with open(args.output, "w") as f:
                json.dump(sync_data, f, indent=2)
            print(f"\n‚úì R√©sultats sauvegard√©s: {args.output}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Erreur de synchronisation: {e}")
        return None


def merge_videos(args):
    """Fusionne les vid√©os en une grille."""
    print("\nüé¨ Fusion des vid√©os...\n")
    
    # Trouver les vid√©os (inclut .mp4 et .MP4)
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    # Cr√©er le dict camera_id -> path
    video_dict = {}
    for i, path in enumerate(video_paths[:args.max_cameras]):
        basename = Path(path).stem
        camera_id = f"cam_{basename.lower().replace(' ', '_')[:20]}"
        video_dict[camera_id] = path
    
    print(f"Fusion de {len(video_dict)} vid√©os...\n")
    
    output = args.output or "merged_output.mp4"
    
    merge_videos_to_grid(
        video_dict,
        output,
        target_fps=args.target_fps,
        max_duration=args.duration,
        show_progress=True
    )
    
    print(f"\n‚úì Vid√©o cr√©√©e: {output}")


def stream_live(args):
    """Affiche les flux en temps r√©el."""
    print("\nüì∫ Mode flux en temps r√©el...\n")
    
    # Charger la configuration
    if args.config:
        with open(args.config) as f:
            config = json.load(f)
    else:
        # Configuration par d√©faut pour les fichiers locaux
        video_paths = expand_video_patterns(args.videos)
        
        def get_rotation_for_video(path):
            """Retourne la rotation √† appliquer selon le fichier vid√©o."""
            basename = Path(path).name.upper()
            # Rotation de -90¬∞ (270¬∞) pour CAMERA_DEVANTURE_PORTE_ENTREE
            if "DEVANTURE_PORTE_ENTREE" in basename:
                return 270
            return 0
        
        config = {
            "cameras": [
                {
                    "camera_id": f"cam_{i}",
                    "name": Path(p).stem,
                    "location": f"Camera {i}",
                    "source": p,
                    "rotation": get_rotation_for_video(p),
                }
                for i, p in enumerate(video_paths[:args.max_cameras])
            ]
        }
    
    if not config.get("cameras"):
        print("‚ùå Aucune cam√©ra configur√©e")
        return
    
    # Cr√©er le gestionnaire
    manager = StreamManager()
    
    # Cr√©er le moniteur de sant√©
    def on_alert(alert):
        level_icon = {
            AlertLevel.INFO: "‚ÑπÔ∏è",
            AlertLevel.WARNING: "‚ö†Ô∏è",
            AlertLevel.ERROR: "‚ùå",
            AlertLevel.CRITICAL: "üö®",
        }
        print(f"{level_icon.get(alert.level, '‚Ä¢')} {alert.camera_id}: {alert.message}")
    
    monitor = HealthMonitor(on_alert=on_alert)
    
    # Ajouter les cam√©ras
    for cam_config in config["cameras"]:
        camera = CameraConfig(
            camera_id=cam_config["camera_id"],
            name=cam_config.get("name", cam_config["camera_id"]),
            location=cam_config.get("location", ""),
            source=cam_config["source"],
            username=cam_config.get("username"),
            password=cam_config.get("password"),
            rotation_degrees=cam_config.get("rotation", 0),
        )
        manager.add_camera(camera)
        monitor.add_camera(camera.camera_id)
    
    # Configurer le merger
    merger = SyncMerger(target_fps=args.target_fps)
    merger.configure_cameras(
        [c["camera_id"] for c in config["cameras"]],
        {c["camera_id"]: c.get("name", c["camera_id"]) for c in config["cameras"]}
    )
    
    print(f"D√©marrage de {len(config['cameras'])} flux...\n")
    print("Contr√¥les: Q=Quitter, R=Enregistrer, S=Screenshot\n")
    
    # Fonction pour r√©cup√©rer les frames
    def get_frames():
        frames = manager.get_all_frames()
        # Rapporter au moniteur
        for cam_id, frame_data in frames.items():
            if frame_data:
                stream = manager.streams.get(cam_id)
                if stream:
                    monitor.report_frame(cam_id, stream.fps_actual)
        return frames
    
    def get_status():
        return {
            cam_id: health.status.value
            for cam_id, health in monitor.cameras.items()
        }
    
    # D√©marrer
    try:
        manager.start_all()
        monitor.start()
        
        # Petit d√©lai pour laisser les flux d√©marrer
        time.sleep(1.0)
        
        # Affichage live
        record_path = args.output if args.record else None
        merger.display_live(
            get_frames,
            window_name="Multi-Camera Surveillance",
            status_source=get_status,
            record_path=record_path
        )
        
    finally:
        merger.stop()
        monitor.stop()
        manager.stop_all()
        
    print("\n‚úì Arr√™t√©")


def sync_live(args):
    """Lecture synchronis√©e bas√©e sur les timestamps d'enregistrement."""
    print("\nüîÑ Mode lecture synchronis√©e par timestamp...\n")
    
    # Trouver les vid√©os
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Vid√©os trouv√©es: {len(video_paths)}\n")
    
    # Cr√©er le lecteur synchronis√©
    enable_track = getattr(args, 'track', False)
    enable_mcmot = getattr(args, 'mcmot', False)
    enable_reid = not getattr(args, 'no_reid', False)
    reid_threshold = getattr(args, 'reid_threshold', 0.6)
    player = TimeSyncedPlayer(
        target_fps=args.target_fps,
        playback_speed=args.speed,
        grid_cell_size=(args.cell_width, args.cell_height),
        align_start=args.align_start,
        enable_tracking=enable_track and not enable_mcmot,
        enable_mcmot=enable_mcmot,
        enable_reid=enable_reid,
        reid_threshold=reid_threshold,
        tracker_model="yolov8n.pt",
    )
    
    # Charger les timestamps manuels si --manual-sync est activ√©
    if args.manual_sync:
        # Timestamps manuels avec corrections de synchronisation
        manual_timestamps = {
            "HALL_PORTE_DROITE": "12:10:10",      # +1s retard (√©tait 12:10:09)
            "HALL_PORTE_GAUCHE": "12:09:55",      # -2s avanc√©
            "DEBUT_COULOIR_DROIT": "12:10:00",    # Retard√© de 5s pour sync avec HALL_PORTE_GAUCHE
            "DEVANTURE_SOUS_ARBRE": "12:10:00",   # -2s avanc√©
            "FIN_COULOIR_GAUCHE_REZ_PARTIE_2": "12:10:03",  # DEBUT
            "FIN_COULOIR_DROIT": "12:10:08",
            "HALL_PORTE_ENTREE": "12:10:27",      # -1s avanc√© (√©tait 12:10:28)
            "DEVANTURE_PORTE_ENTREE": "12:10:34", # -3s avanc√© (√©tait 12:10:37)
            "FIN_COULOIR_GAUCHE_REZ_PARTIE_1": "12:17:33",  # FIN
        }
        
        # Exclusions
        exclude_patterns = ["PARTIE_1(1)", "ESCALIER", "ETAGE1", "FIN_COULOIR_GAUCHE_REZ_FIN"]
        
        player.set_manual_timestamps(manual_timestamps, reference_date="2025-12-11")
        
        # Filtrer les vid√©os √† exclure
        filtered_paths = []
        for vp in video_paths:
            vp_upper = Path(vp).stem.upper()
            excluded = any(pat.upper() in vp_upper for pat in exclude_patterns)
            if not excluded:
                filtered_paths.append(vp)
            else:
                print(f"  ‚è≠Ô∏è Exclu: {Path(vp).name}")
        video_paths = filtered_paths
        print()
    
    # Ajouter les vid√©os avec rotation appropri√©e
    for video_path in video_paths[:args.max_cameras]:
        rotation = get_rotation_for_video(video_path)
        try:
            player.add_video(video_path, rotation_degrees=rotation)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur pour {video_path}: {e}")
    
    if not player.sources:
        print("‚ùå Aucune vid√©o valide")
        return
    
    # Lancer la lecture
    try:
        record_path = getattr(args, 'record', None)
        headless = getattr(args, 'headless', False)
        max_frames = getattr(args, 'max_frames', None)
        player.play(record_output=record_path, headless=headless, max_frames=max_frames)
    except KeyboardInterrupt:
        player.stop()
        print("\n‚ö†Ô∏è Interrompu")


def preprocess_videos(args):
    """Pr√©traite les vid√©os pour cr√©er un dataset coh√©rent."""
    print("\nüîß Pipeline de pr√©traitement vid√©o...\n")
    
    # Trouver les vid√©os
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Vid√©os trouv√©es: {len(video_paths)}\n")
    
    # Cr√©er la configuration
    config = PreprocessingConfig(
        output_format=OutputFormat.MP4_H264,
        target_width=args.width,
        target_height=args.height,
        target_fps=args.fps,
        quality_crf=args.quality,
        enable_stabilization=args.stabilize,
        extract_frames=args.extract_frames,
        frame_interval=args.frame_interval,
        clip_duration=args.clip_duration,
        enable_augmentation=args.augment,
        output_dir=args.output,
        overwrite=args.overwrite,
    )
    
    # Traitement parall√®le ou s√©quentiel
    if args.parallel:
        processor = ParallelProcessor(config, max_workers=args.workers)
        results = processor.process_videos(video_paths)
        summary = processor.get_summary()
        
        print(f"\n{'='*60}")
        print("üìä R√âSUM√â")
        print(f"{'='*60}")
        print(f"   Total: {summary['total']}")
        print(f"   Succ√®s: {summary['success']}")
        print(f"   Erreurs: {summary['errors']}")
        print(f"   Temps total: {summary['total_duration_seconds']:.1f}s")
    else:
        pipeline = PreprocessingPipeline(config)
        
        for i, video_path in enumerate(video_paths):
            print(f"[{i+1}/{len(video_paths)}] {Path(video_path).name}")
            result = pipeline.process_video(video_path)
            status = "‚úì" if result.get("status") == "success" else "‚úó"
            print(f"   {status} -> {result.get('output', 'error')}")
    
    print(f"\n‚úì Pr√©traitement termin√©. Sortie: {args.output}/")


def detect_objects(args):
    """D√©tecte les objets dans les vid√©os."""
    print("\nüîç D√©tection d'objets...\n")
    
    # Trouver les vid√©os
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Vid√©os trouv√©es: {len(video_paths)}\n")
    
    # Cr√©er le d√©tecteur
    detector = create_surveillance_detector(
        model_size=args.model,
        enable_tracking=args.tracking,
        confidence=args.confidence,
    )
    
    # Cr√©er le dossier de sortie
    os.makedirs(args.output, exist_ok=True)
    
    for video_path in video_paths:
        video_name = Path(video_path).stem
        output_path = os.path.join(args.output, f"{video_name}_detected.mp4")
        
        print(f"üìπ {video_name}")
        
        try:
            results = detector.detect_video(
                video_path,
                output_path=output_path,
                show=args.show,
                max_frames=args.max_frames,
            )
            
            stats = detector.get_statistics(results)
            
            print(f"   ‚úì {stats['total_frames']} frames, {stats['total_detections']} d√©tections")
            print(f"   ‚úì {stats['avg_inference_ms']:.1f}ms/frame")
            print(f"   ‚úì Classes: {stats['class_counts']}")
            print(f"   ‚Üí {output_path}\n")
            
        except Exception as e:
            print(f"   ‚úó Erreur: {e}\n")
    
    print(f"‚úì D√©tection termin√©e. Sortie: {args.output}/")


def track_objects(args):
    """Effectue le tracking multi-objets avec trajectoires."""
    print("\nüîç Tracking multi-objets...\n")
    
    # Trouver les vid√©os
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Vid√©os trouv√©es: {len(video_paths)}\n")
    
    # Cr√©er le tracker
    tracker = create_surveillance_tracker(
        model_size=args.model,
        tracker=args.tracker,
        confidence=args.confidence,
    )
    
    # Cr√©er le dossier de sortie
    os.makedirs(args.output, exist_ok=True)
    
    for video_path in video_paths:
        video_name = Path(video_path).stem
        output_path = os.path.join(args.output, f"{video_name}_tracked.mp4")
        
        print(f"üìπ {video_name}")
        
        try:
            # Reset pour chaque vid√©o
            tracker.reset()
            
            results = tracker.track_video(
                video_path,
                output_path=output_path,
                show=args.show,
                max_frames=args.max_frames,
                show_trajectory=not args.no_trajectory,
            )
            
            stats = tracker.get_statistics()
            
            print(f"   ‚úì {stats['total_frames']} frames, {stats['total_tracks']} tracks")
            print(f"   ‚úì Actifs: {stats['active_tracks']}, Perdus: {stats['lost_tracks']}")
            print(f"   ‚úì Classes: {stats['class_distribution']}")
            print(f"   ‚Üí {output_path}\n")
            
        except Exception as e:
            print(f"   ‚úó Erreur: {e}\n")
    
    print(f"‚úì Tracking termin√©. Sortie: {args.output}/")


def mcmot_track(args):
    """Multi-Camera Multi-Object Tracking avec IDs globaux."""
    print("\nüéØ MCMOT - Multi-Camera Tracking...\n")
    
    video_paths = expand_video_patterns(args.videos)
    
    if not video_paths:
        print("‚ùå Aucune vid√©o trouv√©e")
        return
    
    print(f"Cam√©ras: {len(video_paths)}\n")
    
    # Cr√©er le tracker MCMOT
    tracker = MCMOTTracker(
        model_path=f"yolov8{args.model}.pt",
        reid_threshold=args.reid_threshold,
        enable_reid=not args.no_reid,
    )
    
    # Ouvrir les captures
    captures = {}
    for path in video_paths[:4]:  # Max 4 cam√©ras
        camera_id = Path(path).stem
        cap = cv2.VideoCapture(path)
        if cap.isOpened():
            captures[camera_id] = cap
            print(f"  ‚úì {camera_id}")
    
    if not captures:
        print("‚ùå Aucune vid√©o valide")
        return
    
    # Fen√™tre d'affichage
    cv2.namedWindow("MCMOT", cv2.WINDOW_NORMAL)
    
    frame_count = 0
    start_time = time.time()
    
    print(f"\n‚ñ∂Ô∏è D√©marrage du tracking multi-cam√©ra...")
    print("   Contr√¥les: Q=Quitter\n")
    
    try:
        while True:
            timestamp = time.time() - start_time
            frames = {}
            
            # Lire une frame de chaque cam√©ra
            for camera_id, cap in list(captures.items()):
                ret, frame = cap.read()
                if not ret:
                    continue
                frames[camera_id] = frame
            
            if not frames:
                break
            
            if args.max_frames and frame_count >= args.max_frames:
                break
            
            # Tracker chaque cam√©ra
            all_tracks = {}
            for camera_id, frame in frames.items():
                tracks = tracker.process_frame(camera_id, frame, timestamp)
                all_tracks[camera_id] = tracks
            
            # Association cross-camera
            tracker.associate_cross_camera(timestamp)
            
            # Dessiner avec IDs globaux
            annotated_frames = {}
            for camera_id, frame in frames.items():
                annotated = tracker.draw_with_global_ids(
                    camera_id, frame, all_tracks.get(camera_id, [])
                )
                annotated_frames[camera_id] = annotated
            
            # Cr√©er une grille d'affichage
            n = len(annotated_frames)
            cols = 2 if n > 1 else 1
            rows = (n + cols - 1) // cols
            cell_h, cell_w = 360, 480
            grid = np.zeros((rows * cell_h, cols * cell_w, 3), dtype=np.uint8)
            
            for i, (cam_id, frame) in enumerate(annotated_frames.items()):
                row, col = i // cols, i % cols
                x, y = col * cell_w, row * cell_h
                resized = cv2.resize(frame, (cell_w, cell_h))
                grid[y:y+cell_h, x:x+cell_w] = resized
            
            # Afficher stats
            stats = tracker.get_statistics()
            info = f"Global: {stats['total_global_tracks']} | Cross-cam: {stats['cross_camera_tracks']}"
            cv2.putText(grid, info, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow("MCMOT", grid)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            frame_count += 1
    
    finally:
        for cap in captures.values():
            cap.release()
        cv2.destroyAllWindows()
    
    stats = tracker.get_statistics()
    print(f"\n‚úì Termin√©: {frame_count} frames")
    print(f"   Tracks globaux: {stats['total_global_tracks']}")
    print(f"   Tracks cross-camera: {stats['cross_camera_tracks']}")


def create_database(args):
    """Cr√©e et initialise la base de donn√©es."""
    print("\nüíæ Initialisation du stockage...\n")
    
    storage = DataStorage(base_path=args.output)
    
    # Si des vid√©os sont sp√©cifi√©es, les importer
    if args.videos:
        video_paths = expand_video_patterns(args.videos)
        
        if video_paths:
            print(f"Import de {len(video_paths)} vid√©os...")
            metadata_list = extract_all_metadata(video_paths)
            
            for meta in metadata_list:
                # Enregistrer la cam√©ra
                storage.register_camera(
                    camera_id=meta.camera_id,
                    name=meta.camera_id,
                    location=meta.location,
                    source=meta.filepath
                )
                
                # Enregistrer la vid√©o
                record = VideoRecord(
                    id=None,
                    camera_id=meta.camera_id,
                    filename=meta.filename,
                    filepath=meta.filepath,
                    start_time=meta.creation_time or meta.modification_time or "",
                    end_time=None,
                    duration_seconds=meta.duration_seconds,
                    width=meta.width,
                    height=meta.height,
                    fps=meta.fps,
                    frame_count=meta.frame_count,
                    codec=meta.codec,
                    file_size_bytes=meta.file_size_bytes,
                    location=meta.location,
                    has_audio=meta.has_audio,
                    bitrate_kbps=meta.bitrate_kbps,
                )
                storage.add_video_record(record)
            
            print(f"‚úì {len(metadata_list)} vid√©os import√©es")
    
    # Afficher les stats
    stats = storage.get_storage_statistics()
    
    print(f"\n{'='*60}")
    print("üìä STATISTIQUES")
    print(f"{'='*60}")
    print(f"   Vid√©os: {stats['total_videos']}")
    print(f"   Taille: {stats['total_size_gb']:.2f} GB")
    print(f"   Dur√©e: {stats['total_duration_hours']:.1f} heures")
    print(f"   Cam√©ras: {stats['camera_count']}")
    
    # Export
    export_path = storage.export_metadata_json()
    print(f"\n‚úì Base initialis√©e: {storage.db_path}")
    print(f"‚úì Export JSON: {export_path}")
    
    storage.close()


def main():
    """Point d'entr√©e CLI."""
    parser = argparse.ArgumentParser(
        description="Syst√®me de surveillance multi-cam√©ras",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Analyser les vid√©os
  python -m multicam analyze Dataset/*.mp4 -o metadata.json
  
  # Synchroniser les vid√©os
  python -m multicam sync Dataset/*.mp4 --method timestamp -o sync_results.json
  
  # Fusionner en grille
  python -m multicam merge Dataset/*.mp4 -o grid.mp4 --duration 60
  
  # Affichage en temps r√©el
  python -m multicam live Dataset/*.mp4
  
  # Avec flux RTSP
  python -m multicam live --config cameras.json
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Commande")
    
    # Commande: analyze
    p_analyze = subparsers.add_parser("analyze", help="Analyser les m√©tadonn√©es vid√©o")
    p_analyze.add_argument("videos", nargs="+", help="Fichiers vid√©o (glob patterns accept√©s)")
    p_analyze.add_argument("-o", "--output", help="Fichier de sortie JSON")
    p_analyze.set_defaults(func=analyze_videos)
    
    # Commande: sync
    p_sync = subparsers.add_parser("sync", help="Synchroniser les vid√©os")
    p_sync.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_sync.add_argument("-m", "--method", default="timestamp",
                        choices=["timestamp", "audio", "visual"],
                        help="M√©thode de synchronisation")
    p_sync.add_argument("--event-type", default="motion",
                        choices=["flash", "motion"],
                        help="Type d'√©v√©nement pour la m√©thode visuelle")
    p_sync.add_argument("--target-fps", type=float, default=25.0,
                        help="FPS cible")
    p_sync.add_argument("--max-drift-ms", type=float, default=100.0,
                        help="D√©rive max acceptable en ms")
    p_sync.add_argument("-o", "--output", help="Fichier de sortie JSON")
    p_sync.set_defaults(func=synchronize_videos)
    
    # Commande: merge
    p_merge = subparsers.add_parser("merge", help="Fusionner les vid√©os en grille")
    p_merge.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_merge.add_argument("-o", "--output", default="merged.mp4",
                         help="Fichier de sortie")
    p_merge.add_argument("--target-fps", type=float, default=25.0,
                         help="FPS de sortie")
    p_merge.add_argument("--duration", type=float,
                         help="Dur√©e max en secondes")
    p_merge.add_argument("--max-cameras", type=int, default=16,
                         help="Nombre max de cam√©ras")
    p_merge.set_defaults(func=merge_videos)
    
    # Commande: live
    p_live = subparsers.add_parser("live", help="Affichage en temps r√©el")
    p_live.add_argument("videos", nargs="*", help="Fichiers vid√©o ou sources")
    p_live.add_argument("-c", "--config", help="Fichier de configuration JSON")
    p_live.add_argument("--target-fps", type=float, default=25.0,
                        help="FPS cible")
    p_live.add_argument("--max-cameras", type=int, default=16,
                        help="Nombre max de cam√©ras")
    p_live.add_argument("-r", "--record", action="store_true",
                        help="Enregistrer automatiquement")
    p_live.add_argument("-o", "--output", default="recording.mp4",
                        help="Fichier d'enregistrement")
    p_live.set_defaults(func=stream_live)
    
    # Commande: sync-live (lecture synchronis√©e par timestamp)
    p_sync_live = subparsers.add_parser("sync-live", 
        help="Lecture synchronis√©e bas√©e sur les timestamps d'enregistrement")
    p_sync_live.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_sync_live.add_argument("--target-fps", type=float, default=25.0,
                             help="FPS cible")
    p_sync_live.add_argument("--speed", type=float, default=1.0,
                             help="Vitesse de lecture (1.0 = temps r√©el)")
    p_sync_live.add_argument("--max-cameras", type=int, default=16,
                             help="Nombre max de cam√©ras")
    p_sync_live.add_argument("--cell-width", type=int, default=480,
                             help="Largeur des cellules de grille")
    p_sync_live.add_argument("--cell-height", type=int, default=360,
                             help="Hauteur des cellules de grille")
    p_sync_live.add_argument("--align-start", action="store_true",
                             help="Toutes les vid√©os d√©marrent en m√™me temps (ignorer les timestamps)")
    p_sync_live.add_argument("--manual-sync", action="store_true",
                             help="Utiliser les timestamps manuels configur√©s (8 cam√©ras principales)")
    p_sync_live.add_argument("--track", action="store_true",
                             help="Activer le tracking d'objets avec trajectoires")
    p_sync_live.add_argument("--mcmot", action="store_true",
                             help="Activer le tracking cross-camera avec IDs globaux")
    p_sync_live.add_argument("--no-reid", action="store_true",
                             help="D√©sactiver ReID (plus rapide, utilise position)")
    p_sync_live.add_argument("--reid-threshold", type=float, default=0.6,
                             help="Seuil de similarit√© ReID (0-1)")
    p_sync_live.add_argument("--record", type=str, metavar="OUTPUT.mp4",
                             help="Enregistrer le flux synchronis√© dans un fichier vid√©o")
    p_sync_live.add_argument("--headless", action="store_true",
                             help="Mode sans affichage (enregistrement uniquement)")
    p_sync_live.add_argument("--max-frames", type=int,
                             help="Nombre max de frames √† traiter")
    p_sync_live.set_defaults(func=sync_live)
    
    # Commande: preprocess (pr√©traitement vid√©o)
    p_prep = subparsers.add_parser("preprocess", 
        help="Pr√©traiter les vid√©os pour cr√©er un dataset coh√©rent")
    p_prep.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_prep.add_argument("-o", "--output", default="preprocessed",
                        help="Dossier de sortie")
    p_prep.add_argument("--width", type=int, default=1280,
                        help="Largeur cible")
    p_prep.add_argument("--height", type=int, default=720,
                        help="Hauteur cible")
    p_prep.add_argument("--fps", type=float, default=25.0,
                        help="FPS cible")
    p_prep.add_argument("--quality", type=int, default=23,
                        help="Qualit√© CRF (0-51, plus bas = meilleur)")
    p_prep.add_argument("--stabilize", action="store_true",
                        help="Activer la stabilisation d'image")
    p_prep.add_argument("--extract-frames", action="store_true",
                        help="Extraire les frames en images")
    p_prep.add_argument("--frame-interval", type=int, default=1,
                        help="Intervalle d'extraction (1 = toutes les frames)")
    p_prep.add_argument("--clip-duration", type=float,
                        help="D√©couper en clips de N secondes")
    p_prep.add_argument("--augment", action="store_true",
                        help="Appliquer des augmentations de donn√©es")
    p_prep.add_argument("--parallel", action="store_true",
                        help="Traitement parall√®le (multi-process)")
    p_prep.add_argument("--workers", type=int, default=4,
                        help="Nombre de workers pour le traitement parall√®le")
    p_prep.add_argument("--overwrite", action="store_true",
                        help="√âcraser les fichiers existants")
    p_prep.set_defaults(func=preprocess_videos)
    
    # Commande: detect (d√©tection d'objets)
    p_detect = subparsers.add_parser("detect", 
        help="D√©tecter les objets dans les vid√©os (YOLOv8)")
    p_detect.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_detect.add_argument("-o", "--output", default="detections",
                          help="Dossier de sortie")
    p_detect.add_argument("-m", "--model", default="m",
                          choices=["n", "s", "m", "l", "x"],
                          help="Taille du mod√®le (n=nano, s=small, m=medium, l=large, x=xlarge)")
    p_detect.add_argument("-c", "--confidence", type=float, default=0.5,
                          help="Seuil de confiance (0-1)")
    p_detect.add_argument("--tracking", action="store_true",
                          help="Activer le suivi d'objets")
    p_detect.add_argument("--show", action="store_true",
                          help="Afficher en temps r√©el")
    p_detect.add_argument("--max-frames", type=int,
                          help="Nombre max de frames √† traiter")
    p_detect.set_defaults(func=detect_objects)
    
    # Commande: track (tracking multi-objets avec trajectoires)
    p_track = subparsers.add_parser("track",
        help="Tracking multi-objets avec visualisation des trajectoires")
    p_track.add_argument("videos", nargs="+", help="Fichiers vid√©o")
    p_track.add_argument("-o", "--output", default="tracking",
                         help="Dossier de sortie")
    p_track.add_argument("-m", "--model", default="m",
                         choices=["n", "s", "m", "l", "x"],
                         help="Taille du mod√®le YOLO")
    p_track.add_argument("-t", "--tracker", default="bytetrack",
                         choices=["bytetrack", "botsort"],
                         help="Algorithme de tracking")
    p_track.add_argument("-c", "--confidence", type=float, default=0.5,
                         help="Seuil de confiance")
    p_track.add_argument("--show", action="store_true",
                         help="Afficher en temps r√©el")
    p_track.add_argument("--max-frames", type=int,
                         help="Nombre max de frames")
    p_track.add_argument("--no-trajectory", action="store_true",
                         help="D√©sactiver l'affichage des trajectoires")
    p_track.set_defaults(func=track_objects)
    
    # Commande: mcmot (Multi-Camera Multi-Object Tracking)
    p_mcmot = subparsers.add_parser("mcmot",
        help="Multi-Camera Tracking avec IDs globaux cross-camera")
    p_mcmot.add_argument("videos", nargs="+", help="Fichiers vid√©o (max 4)")
    p_mcmot.add_argument("-m", "--model", default="n",
                         choices=["n", "s", "m", "l", "x"],
                         help="Taille du mod√®le YOLO")
    p_mcmot.add_argument("--reid-threshold", type=float, default=0.6,
                         help="Seuil de similarit√© ReID (0-1)")
    p_mcmot.add_argument("--no-reid", action="store_true",
                         help="D√©sactiver ReID (utiliser seulement position)")
    p_mcmot.add_argument("--max-frames", type=int,
                         help="Nombre max de frames")
    p_mcmot.set_defaults(func=mcmot_track)
    
    # Commande: init-db
    p_db = subparsers.add_parser("init-db", help="Initialiser la base de donn√©es")
    p_db.add_argument("-o", "--output", default="output",
                      help="Dossier de sortie")
    p_db.add_argument("videos", nargs="*", help="Vid√©os √† importer")
    p_db.set_defaults(func=create_database)
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Ex√©cuter la commande
    args.func(args)


if __name__ == "__main__":
    main()
